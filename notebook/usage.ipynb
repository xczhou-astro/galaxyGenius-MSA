{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9882e315",
   "metadata": {},
   "source": [
    "# *This notebook is only for illustration and cannot be run.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b2738",
   "metadata": {},
   "source": [
    "## `GalaxyGenius-MSA`\n",
    "\n",
    "`GalaxyGenius-MSA` simulates observations of JWST MSA-3D using TNG hydrodynamical simulations and SKIRT radiative transfer project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253da55",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "- Preprocessing: extract necessary particles, incorporating stars, star-forming regions and dust (if dust is included), from TNG hydrodynamical simulations and save them in particle files. Simultenously, the ski file for SKIRT exection is also generated.\n",
    "- Generation: run SKIRT radiative transfer based on particles files and ski file, and generate data cubes in (wavelength, spatial, spatial) and SEDs.\n",
    "- Postprocess: process the data cube to match the observational conditions of JWST MSA, including dispersion, transmission and resolution. The mock observation is a pseduo-IFU, with output also a data cube. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab415d4d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259dc58",
   "metadata": {},
   "source": [
    "If you do not install `galaxyGeniusMSA` by `pip install .`, you should manually set the base path as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75b080f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # change the path to where galaxyGeniusMSA exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c708c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galaxyGeniusMSA.config import Configuration\n",
    "from galaxyGeniusMSA.preprocess import PreProcess\n",
    "from galaxyGeniusMSA.generation import DataGeneration\n",
    "from galaxyGeniusMSA.postprocess import PostProcess\n",
    "\n",
    "# This is the Units singleton, which determines unit convention for hydro-simulation at different redshift\n",
    "from galaxyGeniusMSA.utils import Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import astropy.units as u\n",
    "from astropy.cosmology import Planck15\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65662e76",
   "metadata": {},
   "source": [
    "### Set paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f9cfc",
   "metadata": {},
   "source": [
    "`GALAXYGENIUS_DATA_DIR` sets the Data director for `galaxyGeniusMSA`, which stores config and SKIRT ski templates. In addition, files for transmission and dispersion are also stored.  \n",
    "`STPSF_PATH` sets the necessary files to generate PSFs for various instrument of JWST.\n",
    "\n",
    "If you do not set environment variables `GALAXYGENIUS_DATA_DIR` and `STPSF_PATH` in `.bashrc`, use following method to set them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GALAXYGENIUS_DATA_DIR'] = 'your/path/to/galaxyGeniusMSA/Data'\n",
    "os.environ['STPSF_PATH'] = 'your/path/to/stpsf-data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea47a9",
   "metadata": {},
   "source": [
    "### Manage configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f6475",
   "metadata": {},
   "source": [
    "configurations control the complete workflow of the simulation. They are divided into two parts, main configuration in `Data/config/config.toml` and mock observation related in `Data/config/config_MSA.toml`. Please check the two files for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf34ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize configuration class\n",
    "config = Configuration()\n",
    "\n",
    "# get configuration\n",
    "conf = config.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d210d",
   "metadata": {},
   "source": [
    "`conf` is a dict, some values are with astropy units, therefore, do not forget the unit when resetting some values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf['includeDust'] = True\n",
    "conf['simulationMode'] = 'ExtinctionOnly'\n",
    "conf['includeVelocity'] = True\n",
    "conf['snapNum'] = 50\n",
    "conf['snapRedshift'] = 1.0\n",
    "conf['viewRedshift'] = 1.0\n",
    "conf['oversample'] = 4\n",
    "\n",
    "conf['starformingSEDFamily'] = 'MAPPINGS'\n",
    "conf['ageThreshold'] = 10 * u.Myr\n",
    "conf['minWaveRT'] = 0.1 * u.um\n",
    "conf['maxWaveRT'] = 2.0 * u.um\n",
    "conf['numWaveRT'] = 3000\n",
    "\n",
    "conf['minWaveOutput'] = 0.97 * u.um\n",
    "conf['maxWaveOutput'] = 1.82 * u.um\n",
    "conf['numWaveOutput'] = 3000\n",
    "\n",
    "conf['faceAndEdge'] = False\n",
    "conf['randomViews'] = False\n",
    "conf['numViews'] = 1\n",
    "conf['inclinations'] = [0]\n",
    "conf['azimuths'] = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a1347",
   "metadata": {},
   "source": [
    "After resetting some configurations, save them in current directory in `.toml` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd09f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.save_config(conf)\n",
    "\n",
    "# reload from the toml file in current directory, not necessary\n",
    "conf = config.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89554328",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d69956",
   "metadata": {},
   "source": [
    "Preprocessing extract particles from hydrodynamical simulations and generate SKIRT ski file.\n",
    "\n",
    "There are four approaches for extraction, corresponding to different data format:  \n",
    "- From snapshots: this approach requires group catalog, snapshot and offset files stored in local. These files are read by `illustris_python`, therefore, the directory tree must follow the demand of `illustris_python`:\n",
    "```Bash\n",
    "    TNGPath  \n",
    "    ├── postprocessing/  \n",
    "    │   └── offsets/  \n",
    "    │       └── offsets_{N}.hdf5  \n",
    "    └── snapshots/  \n",
    "        ├── groups_{N}/  \n",
    "        │   └── fof_subhalo_tab_{N}.{n}.hdf5  \n",
    "        └── Snapshot_{N}/  \n",
    "            └── snap_{N}.{n}.hdf5  \n",
    "```  \n",
    "- From web-based API: this approach is activated by setting `config['requests']` to be `True` and a valid `config['apiKey']` is provided. It employs the web-based API of TNG to retrieve data, therefore, no file mentioned above is required. However, the request speed is slow and may fail sometimes.  \n",
    "- From subhalo particle file: particle file and metadata of each subhalo in each snapshot can be individually downloaded using web-based API. The particle file is much smaller than the complete snapshot. This approach accepts subhalo particle file in `hdf5` format and metadata in `json` format. The subhalo downloading program is in `tutorial/download_subhalos.py`. \n",
    "- Input particles directly: this approach accepts extracted particles already loaded in memory. Note that the units of each property is required. This method is designed for hydrodynamical simulations except Illustris and Illustris-TNG, such as EAGLE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac7d8f",
   "metadata": {},
   "source": [
    "Before initialize `PreProcess` class, the unit convention should be firstly defined, since in TNG simulations, the unit is in `ckpc/h`, `Msun/h` and so on. `c` and `h` represent comoving and hubble constant respectively, which can be different in distinct redshift and cosmological model, and all the calculations in `PreProcess` will follow the defined unit convention.  \n",
    "If you do not define, don't worry, the unit convention will be initialized by Planck15 and the redshift provided by `config['snapRedshift']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units is a singleton. It is the same for all calculations in `PreProcess`\n",
    "units = Units(snapRedshift=conf['snapRedshift'], cosmology=Planck15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PreProcess class\n",
    "preprocess = PreProcess(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087c2e0",
   "metadata": {},
   "source": [
    "- The first approach (from snapshots stored in local)\n",
    "\n",
    "The subhaloIDs and SFRs for subhalos following the mass range defined by `config['minStellarMass']` and `config['maxStellarMass']` are extracted from the snapshot.  \n",
    "And then we specify one subhalo to obtain their metadata and automatically save them in a `workingDir/Subhalo_{subhaloID}.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "subhalos = preprocess.get_subhalos()\n",
    "subhalo_info = preprocess.subhalo(subhalos['SubhaloID'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13e8d7",
   "metadata": {},
   "source": [
    "- The second approach (from web-based API)\n",
    "\n",
    "This approach is activated by setting `config['requests']` to be `True` and a valid `config['apiKey']` is provided.  \n",
    "The subhaloIDs and SFRs for subhalos following the mass range defined by `config['minStellarMass']` and `config['maxStellarMass']` are extracted from the web-based API.  \n",
    "And then we specify one subhalo to obtain their metadata and automatically save them in a `workingDir/Subhalo_{subhaloID}.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "subhalos = preprocess.get_subhalos()\n",
    "subhalo_info = preprocess.subhalo(subhalos['SubhaloID'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad7ba6",
   "metadata": {},
   "source": [
    "- The third approach (from subhalo particle file)\n",
    "\n",
    "This approach should call `inputSubhaloParticleFile` method, which accepts three inputs: `subhaloParticleFile (str)`, `subhaloInfo (dict | None)` and `subhaloInfoFile (str | None)`.  \n",
    "\n",
    "`subhaloParticleFile`: should be in `h5` format downloaded by `tutorial/download_subhalos.py`;  \n",
    "`subhaloInfo`: The required parameters for `subhaloInfo` is `id` (subhaloID), `halfmassrad_stars` (radius at half stellar mass), `pos_x`, `pos_y` and `pos_z` (center position of the subhalo). Other parameters are optional, however it affects the calculation of true properties in `PostProcess`. Consequently, setting this as `None` and input by `subhaloInfoFile` is a better choice;  \n",
    "`subhaloInfoFile` should be in `json` format, which is also downloaded by `tutorial/download_subhalos.py` along with particle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ba6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subhaloID = 0\n",
    "subhalo_file = os.path.join(\n",
    "    f'../data/snapshots-{conf[\"snapNum\"]}', \n",
    "    f'subhalo-{subhaloID}/subhalo_{subhaloID}_particles.h5')\n",
    "subhalo_info_file = os.path.join(\n",
    "    f'../data/snapshots-{conf[\"snapNum\"]}', \n",
    "    f'subhalo-{subhaloID}/subhalo_{subhaloID}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.inputSubhaloParticleFile(subhalo_file, subhaloInfoFile=subhalo_info_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35fdd8",
   "metadata": {},
   "source": [
    "After specifying or loading the data for the considered subhalo, we should call `prepare()` to generate particle files including stars, star-forming regions and dust (if incorporated) and SKIRT ski file.  \n",
    "`prepare()` can take in a dict of some arguments, which corresponds to the configurations, thus the configurations can be modified before generating necessary file for SKIRT. Note that, however, the `config.toml` and `config_MSA.toml` do not change when modifying configurations by this method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}\n",
    "arguments['faceAndEdge'] = False\n",
    "arguments['numViews'] = 1\n",
    "arguments['randomViews'] = False\n",
    "arguments['inclinations'] = [0]\n",
    "arguments['azimuths'] = [0]\n",
    "\n",
    "preprocess.prepare(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13764d2",
   "metadata": {},
   "source": [
    "Once `prepare()` finishes, the following files will appear in the workingDir:  \n",
    "`config.json`: updated configurations and information for subhalo  \n",
    "`dust.txt`: dust particles, if included  \n",
    "`starforming_regions.txt`: star forming regions  \n",
    "`stars.txt`: star particles  \n",
    "`Subhalo_{subhaloID}_particles.h5`: particle properties used in `PreProcess`\n",
    "`Subhalo_{subhaloID}.json`: subhalo metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91573bb",
   "metadata": {},
   "source": [
    "### Data cube generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae8caf",
   "metadata": {},
   "source": [
    "Data cube generation runs SKIRT based on the files mentioned above. The execution takes a well. The outputs are data cube (wavelength, spatial, spatial) and SED for each observing direction. After execution, necessary files are moved to a new directory `dataCubes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fb83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize DataGeneration class\n",
    "dataGeneration = DataGeneration(conf)\n",
    "\n",
    "# run data generation\n",
    "dataGeneration.runSKIRT()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4230e2a",
   "metadata": {},
   "source": [
    "### Postprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a3f2ac",
   "metadata": {},
   "source": [
    "postprocess convert a specific data cube to mock MSA-3D observation. Essentially, this is an IFU simulation, which simulates spectra for each pixel based on the observing conditions of JWST MSA, and assemble them as an IFU data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff804eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PostProcess class\n",
    "postprocess = PostProcess(subhaloID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36b2ea",
   "metadata": {},
   "source": [
    "We define several manual input methods for `PostProcess` class:  \n",
    "\n",
    "`input_bkg_interp(interp_bkg: interp1d)`: input background interpolation function (angstrom vs. MJy/sr). If not called, the interpolation will be obtained by JWST Background Tools (JBT) based on the `obsRA`, `obsDec`, `threshold` and `thisDay` defined in config.\n",
    "\n",
    "`input_psf(psf_cube: np.ndarray)`: input PSF cube (`numpy.ndarray`), make sure that this cube matches the `pixelScale` and `oversamp` in config. If not called, the PSF cube will be obtained using STPSF considering the `disperser`, `filter` and `oversamp` defined in config and wavelength grids for data cube. The PSF modeling process can be slow.\n",
    "\n",
    "`input_subhalo_info(subhalo_info: dict)`: input subhalo info (`dict`), to update the metadata of subhalo.\n",
    "\n",
    "`input_particle_file(particle_file: str)`: input particle filename (`str`), instead of using the one saved by `PreProcess`.  \n",
    "\n",
    "`input_dataCube(dataCube_path: str, viewing_angle: Union[list, None]=None)`, input datacube path (`str`) and the viewing angle (`list`). This method should be called after the previous input methods. If the viewing_angle is set as None, the viewing angle will be obtained from `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data cube, must be called\n",
    "postprocess.input_dataCube(dataCube_path='your/datacube/path/generated/by/SKIRT', \n",
    "                          viewing_angle=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcdfb40",
   "metadata": {},
   "source": [
    "After input the data cube, we are ready to simulate the MSA data tensor. The data tensor will be in 4-dimension (spatial, spatial, 4, wavelength), where the 4 means wavelength, flux, flux error, and SED. Therefore, we technically call it as dataTensor. The dataTensor will be saved in `MSA_mock/Subhalo_{subhaloID}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fbc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess.create_MSA_dataTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18157d9d",
   "metadata": {},
   "source": [
    "Next, we display the MSA mock, including the exposure array, mock observation configuration, several spectra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess.illustrate_MSA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45550003",
   "metadata": {},
   "source": [
    "In order for calibration, we need several properties that can be obtained by fitting spectra and compare with the truth.  \n",
    "We provide several default properties, including 'MetallicityStarFormingRegion', 'MetallicityGas', 'LOSVelocity', 'Mass', 'VelDisp' and 'NumberStarformingRegion'.  \n",
    "You can also define your own properties, the following shows the procedure:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a74c56",
   "metadata": {},
   "source": [
    "1. Define the property function following the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galaxyGeniusMSA.properties import (\n",
    "    age_interp, \n",
    "    rotate_coordinates,\n",
    "    transform,\n",
    "    calc_stats\n",
    ")\n",
    "\n",
    "def property_xx(\n",
    "    particle_file: str, inclination: np.float32, azimuth: np.float32, \n",
    "    bins_perpendicular: np.ndarray, bins_parallel: np.ndarray, \n",
    "    transformation: dict, subhalo_info: dict, configs: dict) -> tuple[np.ndarray, str]:\n",
    "    \n",
    "    \"\"\"\n",
    "    The template function for calculating specific property. \n",
    "    Note that the inner operation can be customized, but the parameters should be kept the same.\n",
    "    The pipeline is as follows:\n",
    "    1. Read the particle file and obtain the properties;\n",
    "    2. Rotate the coordinates to the observing direction;\n",
    "    3. Apply the transformation defined in config;\n",
    "    4. Calculate the statistics for considered properties (e.g. count, mean, std, sum);\n",
    "    5. Return the statistics and unit.\n",
    "    \n",
    "    - particle_file: \n",
    "        The particle file to be used for property. \n",
    "        If postprocess.input_particle_file() is not called, the particle file saved in preprocess will be used.  \n",
    "    - inclination and azimuth:\n",
    "        The inclination and azimuth used in SKIRT, used to rotate the coordinates of particles to the observing direction.\n",
    "    - bins_perpendicular and bins_parallel:\n",
    "        The bins for perpendicular and parallel directions to match the slit stepping and dither. \n",
    "        Note that the units are in kpc. \n",
    "    - transformation:\n",
    "        A dict storing rotate angle, shift in perpendicular and parallel directions.\n",
    "    - subhalo_info:\n",
    "        A dict storing the subhalo information, including SFR, total stellar, DM mass, etc.\n",
    "    - configs:\n",
    "        A dict storing the settings from config.toml and config_MSA.toml.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if need age information\n",
    "    fage = age_interp(configs['cosmology']) \n",
    "    \n",
    "    particles = {}\n",
    "    \n",
    "    # particle file is in h5 format, \n",
    "    # either use the one saved in preprocess or manually input.\n",
    "    with h5py.File(particle_file, 'r') as file:\n",
    "        \n",
    "        # coordinates for stellar particles\n",
    "        # conversion to np.float32 is necessary for numba function to work directly without recompliation\n",
    "        particles['Coordinates'] = file['PartType4']['Coordinates'][:].astype(np.float32)\n",
    "        \n",
    "        # note that for coordinates, the unit should be kpc\n",
    "        # if you use downloaded particle file from TNG, the default unit is ckpc/h \n",
    "        # units.position is ckpc/h, and need to convert to kpc\n",
    "        particles['Coordinates'] = (particles['Coordinates'] * units.position).to_value(u.kpc)\n",
    "        \n",
    "        # if input_particle_file is not called, you use the particle file saved in preprocess,\n",
    "        # the unit is already in kpc and saved as file['partType4']['Coordinates].attrs['unit']\n",
    "        # so you can directly use particles['Coordinates'][:].astype(np.float32), or use\n",
    "        # unit = file['partType4']['Coordinates'].attrs['unit']\n",
    "        # particles['Coordinates] = (particles['Coordinates'] * u.Unit(unit)).to_value(u.kpc)\n",
    "        \n",
    "        # consider properties \n",
    "        particles['Metallicity'] = file['PartType4']['GFM_metallicity'][:].astype(np.float32)\n",
    "        # include other properties as wish, be careful with the unit!!!\n",
    "        \n",
    "    # rotate to observing direction\n",
    "    coords = rotate_coordinates(\n",
    "        coordinates=particles['Coordinates'], \n",
    "        inclination=inclination, \n",
    "        azimuth=azimuth\n",
    "    )\n",
    "    \n",
    "    # apply transformation defined in config\n",
    "    # if rotate and shifts in config are 0, these lines can be removed\n",
    "    coords = transform(coordinates=coords, \n",
    "                       rotate=transformation['rotate'], \n",
    "                       shiftPerpendicular=transformation['shiftPerpendicular'],\n",
    "                       shiftParallel=transformation['shiftParallel'])\n",
    "    \n",
    "    # calculate the statistics for considered properties\n",
    "    stats = calc_stats(\n",
    "        coords=coords, \n",
    "        values=None, # None if for \"count\", otherwise values should be provided\n",
    "        bins_perpendicular=bins_perpendicular, \n",
    "        bins_parallel=bins_parallel, \n",
    "        statistic='count') # statistic type, can be 'count', 'mean', 'std', 'sum'\n",
    "    \n",
    "    # stats = calc_stats(\n",
    "    #     coords=coords,\n",
    "    #     values=particles['Metallicity'],\n",
    "    #     bins_perpendicular=bins_perpendicular,\n",
    "    #     bins_parallel=bins_parallel,\n",
    "    #     statistic='mean'\n",
    "    # )\n",
    "    \n",
    "    unit = '1' # string for unit, if no unit, set to '1'\n",
    "    \n",
    "    return stats, unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdecb5a",
   "metadata": {},
   "source": [
    "2. specify particle file, using the one saved in preprocess or manually input:  \n",
    "    For manually input, call `input_particle_file` method, which accepts a particle file path as input, otherwise, the particle file saved in preprocess will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a576e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling this function will use the particle file specified\n",
    "postprocess.input_particle_file(particle_file='your/particle/file/path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3fe38",
   "metadata": {},
   "source": [
    "3. Input the name of property and the corresponding functions:  \n",
    "`get_truth_properties` method accepts `keep_defaults` (bool), `properties` (list), `functions` (list) as inputs.  \n",
    "\n",
    "    - `keep_defaults`: if true keep the default properties, otherwise, only use the user-defined property functions\n",
    "    - `properties`: list of property names\n",
    "    - `functions`: list of property functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ebafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_defaults = False\n",
    "\n",
    "properties = ['property_xx']\n",
    "functions = [property_xx]\n",
    "\n",
    "postprocess.get_truth_properties(\n",
    "    keep_defaults=keep_defaults, \n",
    "    properties=properties, \n",
    "    functions=functions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8453f054",
   "metadata": {},
   "source": [
    "Finally, the truth properties and their illustrations will be saved in `MSA_mock/Subhalo_{subhaloID}/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78089233",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skirt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
